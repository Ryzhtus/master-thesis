{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "NER with Nonlocal Features BERT Baseline Test.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7Cen33n1xze",
    "outputId": "c0eebbaf-c346-4b28-e2f9-fa170e02d07d"
   },
   "source": [
    "!pip -qq install transformers"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "\u001B[K     |████████████████████████████████| 1.4MB 8.6MB/s \n",
      "\u001B[K     |████████████████████████████████| 2.9MB 25.4MB/s \n",
      "\u001B[K     |████████████████████████████████| 890kB 44.0MB/s \n",
      "\u001B[?25h  Building wheel for sacremoses (setup.py) ... \u001B[?25l\u001B[?25hdone\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Lxb6Fw_u12gD",
    "outputId": "f9e18fe6-5318-4fb3-8d4f-0b2860cba29e"
   },
   "source": [
    "!git clone https://github.com/Ryzhtus/master-thesis"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Cloning into 'master-thesis'...\n",
      "remote: Enumerating objects: 17, done.\u001B[K\n",
      "remote: Counting objects: 100% (17/17), done.\u001B[K\n",
      "remote: Compressing objects: 100% (14/14), done.\u001B[K\n",
      "remote: Total 17 (delta 1), reused 17 (delta 1), pack-reused 0\u001B[K\n",
      "Unpacking objects: 100% (17/17), done.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01uB8xXg16HR",
    "outputId": "ff765ea7-6486-435e-c33f-8bae918cf9f9"
   },
   "source": [
    "cd master-thesis"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "/content/master-thesis\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RmquG0J3MEEm"
   },
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FJu7dUMF2MqJ"
   },
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "class ConLL2003Dataset(Dataset):\n",
    "    def __init__(self, sentences, tags, tags_number, tokenizer):\n",
    "        self.sentences = sentences\n",
    "        self.sentences_tags = tags\n",
    "        self.tags_number = tags_number\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        self.ner_tags = ['<PAD>'] + list(set(tag for tag_list in self.sentences_tags for tag in tag_list))\n",
    "        self.tag2idx = {tag: idx for idx, tag in enumerate(self.ner_tags)}\n",
    "        self.idx2tag = {idx: tag for idx, tag in enumerate(self.ner_tags)}\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        words = self.sentences[item]\n",
    "        tags = self.sentences_tags[item]\n",
    "\n",
    "        tokens = []\n",
    "        for word in words:\n",
    "            if word not in ('[CLS]', '[SEP]'):\n",
    "                tokens.extend(self.tokenizer.tokenize(word))\n",
    "        \n",
    "        tokens = ['[CLS]'] + tokens + ['[SEP]']\n",
    "        tokens_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        tags = tags + ['<PAD>'] * (len(tokens) - len(tags))\n",
    "        tags_ids = [self.tag2idx[tag] for tag in tags]\n",
    "\n",
    "        length = len(tags_ids)\n",
    "\n",
    "        return torch.LongTensor(tokens_ids), torch.LongTensor(tags_ids)\n",
    "\n",
    "    def paddings(self, batch):\n",
    "        tokens, tags = list(zip(*batch))\n",
    "\n",
    "        tokens = pad_sequence(tokens, batch_first=True)\n",
    "        tags = pad_sequence(tags, batch_first=True)\n",
    "\n",
    "        return tokens, tags\n",
    "\n",
    "def read_data(filename):\n",
    "    rows = open(filename, 'r').read().strip().split(\"\\n\\n\")\n",
    "    sentences, sentences_tags = [], []\n",
    "\n",
    "    for sentence in rows:\n",
    "        words = [line.split()[0] for line in sentence.splitlines()]\n",
    "        tags = [line.split()[-1] for line in sentence.splitlines()]\n",
    "        sentences.append(words)\n",
    "        sentences_tags.append(tags)\n",
    "\n",
    "    tags_number = sum([len(tag) for tag in sentences_tags])\n",
    "\n",
    "    return sentences, sentences_tags, tags_number\n",
    "\n",
    "\n",
    "def create_dataset_and_dataloader(filename, batch_size, tokenizer):\n",
    "    sentences, tags, tags_number = read_data(filename)\n",
    "    dataset = ConLL2003Dataset(sentences, tags, tags_number, tokenizer)\n",
    "\n",
    "    return dataset, DataLoader(dataset, batch_size, num_workers=4, collate_fn=dataset.paddings)"
   ],
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R9zjVxBY2TAZ"
   },
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "TOKENIZER = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)\n",
    "DEVICE = 'cuda' if torch.cuda.is_available else 'cpu'\n",
    "EPOCHS = 4\n",
    "BATCH_SIZE = 16"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ydFy8_cW4p01"
   },
   "source": [
    "sentences, tags, tags_number = read_data(\"/content/master-thesis/data/conll2003/train.txt\")\r\n",
    "dataset = ConLL2003Dataset(sentences, tags, tags_number, TOKENIZER)"
   ],
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "XagDapIBQN9V"
   },
   "source": [
    "train_dataset, train_dataloader = create_dataset_and_dataloader(\"/content/master-thesis/data/conll2003/train.txt\", \n",
    "                                                                BATCH_SIZE, TOKENIZER)\n",
    "train_tags_number = train_dataset.tags_number\n",
    "\n",
    "eval_dataset, eval_dataloader = create_dataset_and_dataloader(\"/content/master-thesis/data/conll2003/valid.txt\", \n",
    "                                                              BATCH_SIZE, TOKENIZER)\n",
    "eval_tags_number = eval_dataset.tags_number\n",
    "\n",
    "test_dataset, test_dataloader = create_dataset_and_dataloader(\"/content/master-thesis/data/conll2003/test.txt\", \n",
    "                                                              BATCH_SIZE, TOKENIZER)\n",
    "test_tags_number = test_dataset.tags_number"
   ],
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PMVhyoSpdcGm"
   },
   "source": [
    "from sklearn.metrics import f1_score\r\n",
    "\r\n",
    "def calculate_score(predict_tags, correct_tags):\r\n",
    "    predicted_labels = list(predict_tags.cpu().numpy())\r\n",
    "    correct_labels = list(correct_tags.cpu().numpy())\r\n",
    "\r\n",
    "    predicted_labels_without_mask = []\r\n",
    "    correct_labels_without_mask = []\r\n",
    "    for p, c in zip(predicted_labels, correct_labels):\r\n",
    "        if c > 1:\r\n",
    "            predicted_labels_without_mask.append(p)\r\n",
    "            correct_labels_without_mask.append(c)\r\n",
    "\r\n",
    "    return f1_score(correct_labels_without_mask, predicted_labels_without_mask, average=\"micro\")"
   ],
   "execution_count": 48,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xR_hXXq_dJqP"
   },
   "source": [
    "import torch\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "def train_epoch(model, criterion, optimizer, data, indexer, device):\r\n",
    "    epoch_loss = 0\r\n",
    "    epoch_score = 0\r\n",
    "\r\n",
    "    model.train()\r\n",
    "\r\n",
    "    for batch in data:\r\n",
    "        tokens = batch[0].to(device)\r\n",
    "        tags = batch[1].to(device)\r\n",
    "\r\n",
    "        predictions = model(tokens)\r\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\r\n",
    "        tags_mask = tags != indexer['<PAD>']\r\n",
    "        tags_mask = tags_mask.view(-1)\r\n",
    "        labels = torch.where(tags_mask, tags.view(-1), torch.tensor(criterion.ignore_index).type_as(tags))\r\n",
    "\r\n",
    "        loss = criterion(predictions, labels)\r\n",
    "\r\n",
    "        predictions = predictions.argmax(dim=1, keepdim=True)\r\n",
    "\r\n",
    "        f_score = calculate_score(predictions, labels)\r\n",
    "\r\n",
    "        epoch_loss += loss.item()\r\n",
    "        epoch_score += f_score\r\n",
    "\r\n",
    "        optimizer.zero_grad()\r\n",
    "        loss.backward()\r\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\r\n",
    "        optimizer.step()\r\n",
    "        torch.cuda.empty_cache()\r\n",
    "\r\n",
    "    print('Train Loss = {:.5f}, F1-score = {:.3%}'.format(epoch_loss / len(data), epoch_score / len(data)))\r\n",
    "\r\n",
    "\r\n",
    "def eval_epoch(model, criterion, data, indexer, device):\r\n",
    "    epoch_loss = 0\r\n",
    "    epoch_score = 0\r\n",
    "\r\n",
    "    model.eval()\r\n",
    "\r\n",
    "    with torch.no_grad():\r\n",
    "        for batch in data:\r\n",
    "            tokens = batch[0].to(device)\r\n",
    "            tags = batch[1].to(device)\r\n",
    "\r\n",
    "            predictions = model(tokens)\r\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\r\n",
    "            tags_mask = tags != indexer['<PAD>']\r\n",
    "            tags_mask = tags_mask.view(-1)\r\n",
    "            labels = torch.where(tags_mask, tags.view(-1), torch.tensor(criterion.ignore_index).type_as(tags))\r\n",
    "\r\n",
    "            loss = criterion(predictions, labels)\r\n",
    "\r\n",
    "            predictions = predictions.argmax(dim=1, keepdim=True)\r\n",
    "\r\n",
    "            f_score = calculate_score(predictions, labels)\r\n",
    "\r\n",
    "            epoch_loss += loss.item()\r\n",
    "            epoch_score += f_score\r\n",
    "\r\n",
    "    print('Test Loss = {:.5f}, F1-score = {:.3%}'.format(epoch_loss / len(data), epoch_score / len(data)))\r\n",
    "\r\n",
    "\r\n",
    "def train_model(model, criterion, optimizer, train_data, eval_data, indexer, device, epochs=1):\r\n",
    "    for epoch in range(epochs):\r\n",
    "        print('Epoch {} / {}'.format(epoch + 1, epochs))\r\n",
    "        train_epoch(model, criterion, optimizer, train_data, indexer, device)\r\n",
    "        eval_epoch(model, criterion, eval_data, indexer, device)"
   ],
   "execution_count": 55,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pqlQJlHFdD1N"
   },
   "source": [
    "from transformers import BertModel\r\n",
    "import torch.nn as nn\r\n",
    "\r\n",
    "class BertNER(nn.Module):\r\n",
    "    def __init__(self, num_classes):\r\n",
    "        super(BertNER, self).__init__()\r\n",
    "        self.embedding_dim = 768\r\n",
    "        self.num_classes = num_classes\r\n",
    "\r\n",
    "        self.bert = BertModel.from_pretrained(\"bert-base-cased\")\r\n",
    "        self.linear = nn.Linear(self.embedding_dim, self.num_classes)\r\n",
    "\r\n",
    "    def forward(self, tokens):\r\n",
    "        embeddings = self.bert(tokens)[0]\r\n",
    "        predictions = self.linear(embeddings)\r\n",
    "\r\n",
    "        return predictions"
   ],
   "execution_count": 56,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-vhM2fJdeVD",
    "outputId": "2a3b21d0-9cf8-4c91-d5d6-94ac4fc35ee5"
   },
   "source": [
    "import torch.optim as optim\r\n",
    "from transformers import AdamW\r\n",
    "\r\n",
    "classes = len(dataset.ner_tags)\r\n",
    "\r\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\r\n",
    "\r\n",
    "model = BertNER(classes).to(device)\r\n",
    "\r\n",
    "optimizer = AdamW(model.parameters(), lr=2e-4)\r\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\r\n",
    "\r\n",
    "train_model(model, criterion, optimizer, train_dataloader, eval_dataloader, train_dataset.tag2idx, device, 4)"
   ],
   "execution_count": 57,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Epoch 1 / 4\n",
      "Train Loss = 0.48028, F1-score = 86.706%\n",
      "Test Loss = 0.38025, F1-score = 89.142%\n",
      "Epoch 2 / 4\n",
      "Train Loss = 0.34695, F1-score = 89.769%\n",
      "Test Loss = 0.32511, F1-score = 90.414%\n",
      "Epoch 3 / 4\n",
      "Train Loss = 0.29477, F1-score = 90.893%\n",
      "Test Loss = 0.29785, F1-score = 90.630%\n",
      "Epoch 4 / 4\n",
      "Train Loss = 0.27032, F1-score = 91.441%\n",
      "Test Loss = 0.31808, F1-score = 90.294%\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}